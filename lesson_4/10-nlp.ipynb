{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-23T15:48:31.564676Z","iopub.execute_input":"2023-03-23T15:48:31.566084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastbook import *\nfrom IPython.display import display,HTML","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NLP Deep Dive: RNNs\n- language model is a model that has been trained to guess what the next word in a text is (having read the ones before)\n- called self supervised learning because we do not need to give labels to our model, just fee it lots and lots of text\n- Self supervised learning is not usually used for the model that is trained directly, but instead used for pretraining a model used for transfer learning\n- better results can occur if you fine tune the sequence based langauge model prior to fine tuning the classification model\n    - for instance for IMDB review sentiment analysis we can use 100,000 movie reviews to fine tune the pretrained model that was before trained on wikipedia articles. \n    - this will result in a language model that is particular good at predicting the next word of a movie review and keeping the style consistent. \n\n- Known as Universal Langauge Model Fine-tuning (ULMFit) appraoch\n- an extra stage of fine tuning a language model prior to transfer learning to a classification task resulted in significantly better predictions\n- three stages of transfer learning in NLP\n    - Wikitext Language Model -> IMDb Language Model -> IMDb Classifier","metadata":{}},{"cell_type":"markdown","source":"## Text Preprocessing\n- Approach we take for single categorical variable\n    - make a list of all possible levels of that categorical variable (called vocab)\n    - replace each level with its index in the vocab\n    - create an embedding matrix for this containing a row for each level (each item of the vocab)\n    - use this embedding matrix as the first layer of a neural net\n\n- The same can be done with text\n- Concatenate all documents in our dataset into a long string and split it into words (tokens)\n- Our independent variable will be the sequence of words starting with the frist in our long list and ending with the second to last\n- the dependent variable will be the sequence of words starting with the second word and ending with the last word\n- vocab will consist of a mix of common words that are already in the vocabulary of the pretrained model and new words specific to our corpus (for imdb example actor names or cinematogrphic terms)\n- For words in the vocab of our pretrained model we will take the corresponding row in the embedding matrix. New words will be initialized with a random vector","metadata":{}},{"cell_type":"markdown","source":"- steps necessary to create a language model\n    - Tokenization (converting text into a list of words)\n    - Numericalization (make a list of all unique words that appear (vocab), and convert each word into a number by looking up its index in the vocab\n    - Language model data loader creation\n        - Fastai provides a LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token.\n        - also handles details such as how to shuffle the training data in a way that the dependent and independent variables maintain their structure as required\n     - Language model creation\n         - using a recurrent neural network","metadata":{}},{"cell_type":"markdown","source":"## Tokenization\n- three main appraoches\n    - word-based: split sentence on spaces while applying language speicifc rules to try to seperate parts of meaning when there are no spaces.\n    - Subword based: split words into smaller parts based on the most commonly occuring substrings. \"occasion\" might be \"o c ca sion\".\n    - Character-based: split sentence into individual characters","metadata":{}},{"cell_type":"code","source":"# Word tokenization with fastai\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grab the text files\nfiles = get_text_files(path, folders = [\"train\", \"test\", \"unsup\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt = files[0].open().read(); txt[:75]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use WordTokenizer to create tokens\nspacy = WordTokenizer()\ntoks = first(spacy([txt]))\nprint(coll_repr(toks, 30))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fastai adds additional functionality to the tokenization process with the Tokenizer class\ntkn = Tokenizer(spacy)\nprint(coll_repr(tkn(txt), 31))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# those starting with \"xx\" are special tokens\n# xxbos indicates start of new text (beginning of stream)\n# this lets the model know if it needs to forget what was said before (given it is the start of a new stream)\n\n# xxmaj indicates the next word begins with a capital (we lowercased everything before)\n# xxunk indicates the word is unknown\n\n# see default rules\ndefaults.text_proc_rules","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Subword Tokenization\n# assumption spaces provide a useful separation of components of meaning in a sentence\n# best used in cases where languages do not have spaces (chinese) or use little spaces (hungarian)\n# two steps\n    # analyze corpus of documents to find the most commonly occuring groups of letters (these become vocab)\n    # tokenize the corpus using this vocab ov subword units","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we instantiate our tokenizer passing in the size of the vocab\n# we need to train it or have it read out docs to find common sequences\n#txts = L(o.open().read() for o in files[:2000])\n\n# remove bad character file\nfiles.pop(58)\ntxts = L(open(o, encoding=\"utf8\").read() for o in files[58:59])\n\n# training is done with setup\n# setup is a fastai method that is called automatically in our data processing pipelines\n# we have to call it ourself since we are doing this manually\ndef subword(sz):\n    sp = SubwordTokenizer(vocab_sz=sz)\n    sp.setup(txts)\n    return ' '.join(first(sp([txt]))[:40])\n\nsubword(1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if a smaller vocab is used each token will represent fewer characters\nsubword(200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# picking a subword size represents a compromise\n# larger vocab means fewer tokens per sentence which means faster training, less memory and less state for the model to remember\n# the downside is larger embedding matrices which require more data\n\n# subword tokenization provides a way to easily scale between character tokenization and word tokenization\n# last year has gotten more popular","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Numericalization with fastai\n# mapping tokens to integers\n    # make a list of all possible levels of that categorical variable (vocab)\n    # replace each level with its index in the vocab\ntoks = tkn(txt)\nprint(coll_repr(tkn(txt), 31))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we need to call setup to create the vocab\n# we need our tokenized corpus first\n# since tokenization takes a while this example will use a small subset\ntoks200 = txts[:200].map(tkn)\ntoks200[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num = Numericalize()\nnum.setup(toks200)\ncoll_repr(num.vocab,20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# special rules tokens appear first, then every word appears once in frequency order\n# defaults to Numericalize is min_freq=3, max_vocab=60000\n# Once Numericalize is created we can use it as a function\nnums = num(toks)[:20]\nnums","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokens have been converted to a tensor of integers that our model can recieve\n# check that they map back to the original text\n\" \".join(num.vocab[o] for o in nums)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now that we have numbers we need to put them in batches for our model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Putting our Texts into Batches for a Language Model\n- when dealing with images we need to resize them all to the same height and width before batching\n- Here is different because we can't resize text\n- We also need to be careful with order since we wan't our model to read text in order so it can predict the next word","metadata":{}},{"cell_type":"code","source":"# 90 tokens batched into 6 lengths of 15\nstream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\ntokens = tkn(stream)\nbs,seq_len = 6,15\nd_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In a perfect world we could give this one batch to our model, but this doesn't scale\n# Unlikely a single batch containing several million lines of text would fit into GPU memory\n# we need to divide this array more finely into subarrays\n    # the model we use will maintain a state so that it remembers what it read previously when predicting what comes next\n\n# choose a sequence length of 5, we first feed the following array\nbs, seq_len = 6, 5\nd_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False, header=None)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# then this one\n#hide_input\nbs,seq_len = 6,5\nd_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finally\n#hide_input\nbs,seq_len = 6,5\nd_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])\ndf = pd.DataFrame(d_tokens)\ndisplay(HTML(df.to_html(index=False,header=None)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the first step is to transform the individual texts into a stream by concatenating them together\n# we then cut this stream into batches\n# if we have 50,000 tokens and set a batch size to 10 we need 10 mini streams of 5000 tokens\n# we must preserve the order of the tokens so the model reads continous rows of text\n\n# all done with the fastai library LMDataLoader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply Numericalize object to the tokenized text\nnums200 = toks200.map(num)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pass to LMDataLoader\ndl = LMDataLoader(nums200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check we can grab the first batch\nx, y = first(dl)\nx.shape, y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at the first row of independent variable to see the start of the first text\n\" \" .join(num.vocab[o] for o in x[0][:20])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the dependent variable is the same thing offset by one token\n\" \".join(num.vocab[o] for o in y[0][:20])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training a Text Classifier\n- two steps\n    - fine tune our language model pretrainined on Wikipedia to the corpus of IMDB reviews\n    - use that model to train a classifier","metadata":{}},{"cell_type":"code","source":"# Assemble data\n# Language Model using DataBlock\n# fastai handles tokenization and numericalization when TextBlock is passed to DataBlock\n\nget_imdb = partial(get_text_files, folders=[\"train\", \"test\", \"unsup\"])\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb, splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=128, seq_len=80)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we are not just using TextBlock directly but calling a class method\n# TextBlock is special because it sets up the numericalizer's vocab that can take a long time\n# It performs optimizations to save time such as \n    # saving tokenized dcouments in a temp folder, so it doesn't hav eto tokenize them more than once\n    # runs multiple tokenization processes in parallel, to take advantage of a CPU\n\n# We tell TextBlock how to access texts so it can do this preprocessing\n# that's what from_folder does","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show batch\ndls_lm.show_batch(max_n=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fine-Tuning the Language Model\n# to convert the integer word indicies into activations for our neural network we use embeddings\n# we feed those embeddings into a recurent neural network using an architecture called AWD-LSTM\n# embedding in the pretrained model are merged with random embeddings added for words that weren't in the pretraining vocabulary\n# this is handled automatically inside language_model_learner","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = language_model_learner(\n    dls_lm, AWD_LSTM, drop_mult=0.3, \n    metrics=[accuracy, Perplexity()]).to_fp16()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss function used by default is cross-entropy loss since we have a classification problem\n# perplexity metric used here is often used with NLP for language models\n# it is the exponential of the loss torch.exp(cross_entropy)\n# we also include accuracy to see how many times our model is right when trying to predict the next word","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# going back to our process diagram we have completed the first step (pretrained model in fastai)\n# we have built the dataloaders and learner for the second step of tuning to the corpus of IMDB","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training each epoch takes a while so we save the intermediate model results during the training process\n# we use fit_one_cycle to do that for us\n# language_model_learner  automatically calls freeze when using a pretrained model\n# this will only train the embeddings (the only part of the model that contains randomly initialized weights ie embedding for words that are in our IMDB vocab, but aren't in the pretrained model vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fit_one_cycle(1, 2e-2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}