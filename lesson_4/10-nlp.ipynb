{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-17T19:28:29.651565Z","iopub.execute_input":"2023-03-17T19:28:29.652474Z","iopub.status.idle":"2023-03-17T19:28:49.005335Z","shell.execute_reply.started":"2023-03-17T19:28:29.652415Z","shell.execute_reply":"2023-03-17T19:28:49.003715Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from fastbook import *\nfrom IPython.display import display,HTML","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:28:49.008536Z","iopub.execute_input":"2023-03-17T19:28:49.009031Z","iopub.status.idle":"2023-03-17T19:28:49.016843Z","shell.execute_reply.started":"2023-03-17T19:28:49.008970Z","shell.execute_reply":"2023-03-17T19:28:49.015709Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## NLP Deep Dive: RNNs\n- language model is a model that has been trained to guess what the next word in a text is (having read the ones before)\n- called self supervised learning because we do not need to give labels to our model, just fee it lots and lots of text\n- Self supervised learning is not usually used for the model that is trained directly, but instead used for pretraining a model used for transfer learning\n- better results can occur if you fine tune the sequence based langauge model prior to fine tuning the classification model\n    - for instance for IMDB review sentiment analysis we can use 100,000 movie reviews to fine tune the pretrained model that was before trained on wikipedia articles. \n    - this will result in a language model that is particular good at predicting the next word of a movie review and keeping the style consistent. \n\n- Known as Universal Langauge Model Fine-tuning (ULMFit) appraoch\n- an extra stage of fine tuning a language model prior to transfer learning to a classification task resulted in significantly better predictions\n- three stages of transfer learning in NLP\n    - Wikitext Language Model -> IMDb Language Model -> IMDb Classifier","metadata":{}},{"cell_type":"markdown","source":"## Text Preprocessing\n- Approach we take for single categorical variable\n    - make a list of all possible levels of that categorical variable (called vocab)\n    - replace each level with its index in the vocab\n    - create an embedding matrix for this containing a row for each level (each item of the vocab)\n    - use this embedding matrix as the first layer of a neural net\n\n- The same can be done with text\n- Concatenate all documents in our dataset into a long string and split it into words (tokens)\n- Our independent variable will be the sequence of words starting with the frist in our long list and ending with the second to last\n- the dependent variable will be the sequence of words starting with the second word and ending with the last word\n- vocab will consist of a mix of common words that are already in the vocabulary of the pretrained model and new words specific to our corpus (for imdb example actor names or cinematogrphic terms)\n- For words in the vocab of our pretrained model we will take the corresponding row in the embedding matrix. New words will be initialized with a random vector","metadata":{}},{"cell_type":"markdown","source":"- steps necessary to create a language model\n    - Tokenization (converting text into a list of words)\n    - Numericalization (make a list of all unique words that appear (vocab), and convert each word into a number by looking up its index in the vocab\n    - Language model data loader creation\n        - Fastai provides a LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token.\n        - also handles details such as how to shuffle the training data in a way that the dependent and independent variables maintain their structure as required\n     - Language model creation\n         - using a recurrent neural network","metadata":{}},{"cell_type":"markdown","source":"## Tokenization\n- three main appraoches\n    - word-based: split sentence on spaces while applying language speicifc rules to try to seperate parts of meaning when there are no spaces.\n    - Subword based: split words into smaller parts based on the most commonly occuring substrings. \"occasion\" might be \"o c ca sion\".\n    - Character-based: split sentence into individual characters","metadata":{}},{"cell_type":"code","source":"# Word tokenization with fastai\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:28:49.018570Z","iopub.execute_input":"2023-03-17T19:28:49.020503Z","iopub.status.idle":"2023-03-17T19:29:31.896653Z","shell.execute_reply.started":"2023-03-17T19:28:49.020453Z","shell.execute_reply":"2023-03-17T19:29:31.895545Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [144441344/144440600 00:10&lt;00:00]\n    </div>\n    "},"metadata":{}}]},{"cell_type":"code","source":"# grab the text files\nfiles = get_text_files(path, folders = [\"train\", \"test\", \"unsup\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:29:31.900157Z","iopub.execute_input":"2023-03-17T19:29:31.901379Z","iopub.status.idle":"2023-03-17T19:29:32.885898Z","shell.execute_reply.started":"2023-03-17T19:29:31.901327Z","shell.execute_reply":"2023-03-17T19:29:32.884890Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"txt = files[0].open().read(); txt[:75]","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:29:32.887921Z","iopub.execute_input":"2023-03-17T19:29:32.888793Z","iopub.status.idle":"2023-03-17T19:29:32.898456Z","shell.execute_reply.started":"2023-03-17T19:29:32.888743Z","shell.execute_reply":"2023-03-17T19:29:32.896913Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'Jiang Xian uses the complex backstory of Ling Ling and Mao Daobing to study'"},"metadata":{}}]},{"cell_type":"code","source":"# Use WordTokenizer to create tokens\nspacy = WordTokenizer()\ntoks = first(spacy([txt]))\nprint(coll_repr(toks, 30))","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:29:32.900673Z","iopub.execute_input":"2023-03-17T19:29:32.901293Z","iopub.status.idle":"2023-03-17T19:29:45.233859Z","shell.execute_reply.started":"2023-03-17T19:29:32.901244Z","shell.execute_reply":"2023-03-17T19:29:45.232608Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(#143) ['Jiang','Xian','uses','the','complex','backstory','of','Ling','Ling','and','Mao','Daobing','to','study','Mao',\"'s\",'\"','cultural','revolution','\"','(','1966','-','1976',')','at','the','village','level','.'...]\n","output_type":"stream"}]},{"cell_type":"code","source":"# fastai adds additional functionality to the tokenization process with the Tokenizer class\ntkn = Tokenizer(spacy)\nprint(coll_repr(tkn(txt), 31))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:29:45.235816Z","iopub.execute_input":"2023-03-17T19:29:45.236716Z","iopub.status.idle":"2023-03-17T19:29:45.251202Z","shell.execute_reply.started":"2023-03-17T19:29:45.236643Z","shell.execute_reply":"2023-03-17T19:29:45.248657Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(#158) ['xxbos','xxmaj','jiang','xxmaj','xian','uses','the','complex','backstory','of','xxmaj','ling','xxmaj','ling','and','xxmaj','mao','xxmaj','daobing','to','study','xxmaj','mao',\"'s\",'\"','cultural','revolution','\"','(','1966','-'...]\n","output_type":"stream"}]},{"cell_type":"code","source":"# those starting with \"xx\" are special tokens\n# xxbos indicates start of new text (beginning of stream)\n# this lets the model know if it needs to forget what was said before (given it is the start of a new stream)\n\n# xxmaj indicates the next word begins with a capital (we lowercased everything before)\n# xxunk indicates the word is unknown\n\n# see default rules\ndefaults.text_proc_rules","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:29:45.257256Z","iopub.execute_input":"2023-03-17T19:29:45.260352Z","iopub.status.idle":"2023-03-17T19:29:45.401334Z","shell.execute_reply.started":"2023-03-17T19:29:45.260234Z","shell.execute_reply":"2023-03-17T19:29:45.399501Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[<function fastai.text.core.fix_html(x)>,\n <function fastai.text.core.replace_rep(t)>,\n <function fastai.text.core.replace_wrep(t)>,\n <function fastai.text.core.spec_add_spaces(t)>,\n <function fastai.text.core.rm_useless_spaces(t)>,\n <function fastai.text.core.replace_all_caps(t)>,\n <function fastai.text.core.replace_maj(t)>,\n <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"},"metadata":{}}]},{"cell_type":"code","source":"# Subword Tokenization\n# assumption spaces provide a useful separation of components of meaning in a sentence\n# best used in cases where languages do not have spaces (chinese) or use little spaces (hungarian)\n# two steps\n    # analyze corpus of documents to find the most commonly occuring groups of letters (these become vocab)\n    # tokenize the corpus using this vocab ov subword units","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:29:45.406215Z","iopub.execute_input":"2023-03-17T19:29:45.407490Z","iopub.status.idle":"2023-03-17T19:29:45.454695Z","shell.execute_reply.started":"2023-03-17T19:29:45.407426Z","shell.execute_reply":"2023-03-17T19:29:45.452832Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# we instantiate our tokenizer passing in the size of the vocab\n# we need to train it or have it read out docs to find common sequences\ntxts = L(o.open().read() for o in files[:2000])\n\n# training is done with setup\n# setup is a fastai method that is called automatically in our data processing pipelines\n# we have to call it ourself since we are doing this manually\ndef subword(sz):\n    sp = SubwordTokenizer(vocab_sz=sz)\n    sp.setup(txts)\n    return \" \".join(first(sp([txt]))[:40])\n\nsubword(1000)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:29:45.458767Z","iopub.execute_input":"2023-03-17T19:29:45.459824Z","iopub.status.idle":"2023-03-17T19:29:54.024564Z","shell.execute_reply.started":"2023-03-17T19:29:45.459762Z","shell.execute_reply":"2023-03-17T19:29:54.023657Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tmp/texts.out --vocab_size=1000 --model_prefix=tmp/spm --character_coverage=0.99999 --model_type=unigram --unk_id=9 --pad_id=-1 --bos_id=-1 --eos_id=-1 --minloglevel=2 --user_defined_symbols=▁xxunk,▁xxpad,▁xxbos,▁xxeos,▁xxfld,▁xxrep,▁xxwrep,▁xxup,▁xxmaj --hard_vocab_limit=false\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'▁J i ang ▁ X ian ▁us es ▁the ▁comp le x ▁back st or y ▁of ▁L ing ▁L ing ▁and ▁Ma o ▁Da o b ing ▁to ▁st u d y ▁Ma o \\' s ▁\" c ul'"},"metadata":{}}]},{"cell_type":"code","source":"# if a smaller vocab is used each token will represent fewer characters\nsubword(200)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:36:40.261521Z","iopub.execute_input":"2023-03-17T19:36:40.262123Z","iopub.status.idle":"2023-03-17T19:36:50.204582Z","shell.execute_reply.started":"2023-03-17T19:36:40.262054Z","shell.execute_reply":"2023-03-17T19:36:50.203124Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'▁ J i an g ▁ X i an ▁ us es ▁the ▁c o m p le x ▁b a ck st or y ▁of ▁ L ing ▁ L ing ▁and ▁ M a o ▁ D a'"},"metadata":{}}]},{"cell_type":"code","source":"# picking a subword size represents a compromise\n# larger vocab means fewer tokens per sentence which means faster training, less memory and less state for the model to remember\n# the downside is larger embedding matrices which require more data\n\n# subword tokenization provides a way to easily scale between character tokenization and word tokenization\n# last year has gotten more popular","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:38:15.213834Z","iopub.execute_input":"2023-03-17T19:38:15.214282Z","iopub.status.idle":"2023-03-17T19:38:15.220042Z","shell.execute_reply.started":"2023-03-17T19:38:15.214241Z","shell.execute_reply":"2023-03-17T19:38:15.218351Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Numericalization with fastai\n# mapping tokens to integers\n    # make a list of all possible levels of that categorical variable (vocab)\n    # replace each level with its index in the vocab\ntoks = tkn(txt)\nprint(coll_repr(tkn(txt), 31))","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:39:12.141943Z","iopub.execute_input":"2023-03-17T19:39:12.142616Z","iopub.status.idle":"2023-03-17T19:39:12.152416Z","shell.execute_reply.started":"2023-03-17T19:39:12.142572Z","shell.execute_reply":"2023-03-17T19:39:12.151185Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(#158) ['xxbos','xxmaj','jiang','xxmaj','xian','uses','the','complex','backstory','of','xxmaj','ling','xxmaj','ling','and','xxmaj','mao','xxmaj','daobing','to','study','xxmaj','mao',\"'s\",'\"','cultural','revolution','\"','(','1966','-'...]\n","output_type":"stream"}]},{"cell_type":"code","source":"# we need to call setup to create the vocab\n# we need our tokenized corpus first\n# since tokenization takes a while this example will use a small subset\ntoks200 = txts[:200].map(tkn)\ntoks200[0]","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:40:33.705616Z","iopub.execute_input":"2023-03-17T19:40:33.706044Z","iopub.status.idle":"2023-03-17T19:40:34.757225Z","shell.execute_reply.started":"2023-03-17T19:40:33.706006Z","shell.execute_reply":"2023-03-17T19:40:34.755901Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(#158) ['xxbos','xxmaj','jiang','xxmaj','xian','uses','the','complex','backstory','of'...]"},"metadata":{}}]},{"cell_type":"code","source":"num = Numericalize()\nnum.setup(toks200)\ncoll_repr(num.vocab,20)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:40:53.337421Z","iopub.execute_input":"2023-03-17T19:40:53.338785Z","iopub.status.idle":"2023-03-17T19:40:53.365352Z","shell.execute_reply.started":"2023-03-17T19:40:53.338727Z","shell.execute_reply":"2023-03-17T19:40:53.363932Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"\"(#2152) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','in','it','i'...]\""},"metadata":{}}]},{"cell_type":"code","source":"# special rules tokens appear first, then every word appears once in frequency order\n# defaults to Numericalize is min_freq=3, max_vocab=60000\n# Once Numericalize is created we can use it as a function\nnums = num(toks)[:20]\nnums","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:42:36.648431Z","iopub.execute_input":"2023-03-17T19:42:36.648997Z","iopub.status.idle":"2023-03-17T19:42:36.703655Z","shell.execute_reply.started":"2023-03-17T19:42:36.648948Z","shell.execute_reply":"2023-03-17T19:42:36.702340Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TensorText([   2,    8,    0,    8,    0, 1269,    9, 1270,    0,   14,    8,    0,    8,    0,   12,    8,    0,    8,    0,   15])"},"metadata":{}}]},{"cell_type":"code","source":"# tokens have been converted to a tensor of integers that our model can recieve\n# check that they map back to the original text\n\" \".join(num.vocab[o] for o in nums)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:43:31.619264Z","iopub.execute_input":"2023-03-17T19:43:31.620120Z","iopub.status.idle":"2023-03-17T19:43:31.629061Z","shell.execute_reply.started":"2023-03-17T19:43:31.620056Z","shell.execute_reply":"2023-03-17T19:43:31.627806Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'xxbos xxmaj xxunk xxmaj xxunk uses the complex xxunk of xxmaj xxunk xxmaj xxunk and xxmaj xxunk xxmaj xxunk to'"},"metadata":{}}]},{"cell_type":"code","source":"# now that we have numbers we need to put them in batches for our model","metadata":{"execution":{"iopub.status.busy":"2023-03-17T19:44:12.299499Z","iopub.execute_input":"2023-03-17T19:44:12.300055Z","iopub.status.idle":"2023-03-17T19:44:12.305370Z","shell.execute_reply.started":"2023-03-17T19:44:12.300005Z","shell.execute_reply":"2023-03-17T19:44:12.304433Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Putting our Texts into Batches for a Language Model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}